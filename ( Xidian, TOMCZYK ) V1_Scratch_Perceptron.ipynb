{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f8a8193",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# PERCEPTRON\n",
    "***\n",
    "***\n",
    "author --- louis tomczyk<br>\n",
    "institution --- Xidian University<br>\n",
    "student id --- 211.561.13.752<br>\n",
    "date --- 2021.11.08 --- 10.02pm<br>\n",
    "version --- V1\n",
    "course --- X2 CS 10 26 - Machine Learning<br>\n",
    "contact --- <louis.tomczyk.work@gmail.com><br>\n",
    "bibliography --- <br>\n",
    "- __[Multi-Class Classification and the Perceptron](https://jermwatt.github.io/machine_learning_refined/notes/7_Linear_multiclass_classification/7_3_Perceptron.html)__<br>\n",
    "- __[implementation perceptron](https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/)__<br>\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3cf5e",
   "metadata": {},
   "source": [
    "## 0/ Maintenance\n",
    "***\n",
    "the notations are :\n",
    "- $file\\_ name$ : the name of the file containing the dataset\n",
    "- $df$ : the *DataFrame* containing the imported dataset\n",
    "- $data$ : the imported data into a *numpy array* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cba530",
   "metadata": {},
   "source": [
    "### 0.1/ librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb36390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for mathematical operations\n",
    "import numpy as np\n",
    "\n",
    "# for importing data from csv\n",
    "import pandas as pd\n",
    "\n",
    "# for spliting the dataset in n-subdatasets\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38b69b",
   "metadata": {},
   "source": [
    "### 0.2/ importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac8e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name   = \"Data.csv\"\n",
    "df          = pd.read_csv(file_name)\n",
    "data        = df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d7979",
   "metadata": {},
   "source": [
    "## 1/ Data\n",
    "***\n",
    "\n",
    "the notations are :\n",
    "- $n_{samples}$ : the number of samples in the dataset\n",
    "- $n_{features}$ : the number of features used to desribe the samples\n",
    "- $x$ : the features matrix made of  $(n_{samples})$ rows and $(n_{features})$ columns\n",
    "- $y$ : the targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a5a432",
   "metadata": {},
   "source": [
    "### 1.1/ format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c5167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x           = np.array([data.T[0], data.T[1]]).T\n",
    "y           = data.T[2]\n",
    "y           = y[np.newaxis,:].T\n",
    "n_samples   = len(y)\n",
    "n_features  = len(x[0])\n",
    "\n",
    "# images resolution\n",
    "resolution  = 2000\n",
    "draw = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55160c08",
   "metadata": {},
   "source": [
    "### 1.2/ visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de793a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "if draw == 1:\n",
    "    plt.figure(dpi=resolution)\n",
    "    plt.scatter(x.T[0],x.T[1],s=100,c=y.flatten())\n",
    "    plt.xlabel(\"$x[0] = x_1$\")\n",
    "    plt.ylabel(\"$x[1] = x_2$\")\n",
    "    plt.title(\"data visualisation\")\n",
    "    plt.savefig(\"Initial_dataset.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1df57d",
   "metadata": {},
   "source": [
    "## 2/ Model : Single Layer Perceptron\n",
    "***\n",
    "The notations are :\n",
    "- $w$ : weights\n",
    "- $b$ : bias \n",
    "- $a$ : the activation value at the neuron output\n",
    "\n",
    "The functions are : \n",
    "- *initialisation* : default values for the weights\n",
    "- *model* : the activation of the neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2585e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs : X - the features for each sample in the dataset\n",
    "# output : the defaults values for weights (W) and bias (b)\n",
    "\n",
    "def initialisation(x,method):\n",
    "\n",
    "\n",
    "    if method == 'null':\n",
    "        b = np.zeros(1)\n",
    "        w = np.zeros((x.shape[1],1))\n",
    "\n",
    "    if method == 'rand':\n",
    "        b = np.random.randn(1)\n",
    "        w = np.random.randn(x.shape[1], 1)\n",
    "\n",
    "    return (b,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43394057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs : \n",
    "#   X - the features for each sample in the dataset\n",
    "#   W - the weights\n",
    "#   b - the bias\n",
    "\n",
    "# outputs :\n",
    "#   A - the activation of the neuron\n",
    "\n",
    "def model(x,b,w):\n",
    "\n",
    "    z = x.dot(w) + b\n",
    "    a = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3305cb",
   "metadata": {},
   "source": [
    "## 3/ Cost function\n",
    "***\n",
    "the notations are :\n",
    "- $\\mathcal{LL}$ : the logarithm of the losses calcualted upon the likelihood theory\n",
    "\n",
    "the functions are :\n",
    "- *log_loss* : to calculate the cross-entropy which is defined as : <br>\n",
    "$\\mathcal{LL}(a,y) = \\dfrac{1}{n_{samples}}\\cdot \\sum_{k=1}^{n_{samples}}\\ \\bigg[y[k]\\cdot ln\\big(a[k])+(1-y[k]\\big)\\cdot ln\\big(1-a[k]\\big)\\bigg]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "870422ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs :\n",
    "#   a - the activation functions for the neurons\n",
    "#   y - the targets\n",
    "\n",
    "# outputs :\n",
    "#   the cross-entropy\n",
    "\n",
    "def log_loss(a, y):\n",
    "    return 1/n_samples*np.sum(-y*np.log(a)-(1-y)*np.log(1-a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23cf7df",
   "metadata": {},
   "source": [
    "## 4/ Gradients\n",
    "***\n",
    "the functions are : <br>\n",
    "- *gradients* : to calculate the correction of the weights given by the gradient of the cross-entropy :\n",
    "    - $\\delta w = \\vec{grad}_{w}\\big(\\mathcal{LL}\\big)=\\dfrac{1}{n}\\cdot \\sum_{k=1}^{n_{samples}}\\ \\Bigg[\\bigg(a(x[k])-y[k])\\bigg)\\cdot x[k]\\Bigg]$\n",
    "    - $\\delta b = \\vec{grad}_{b}\\big(\\mathcal{LL}\\big) = \\dfrac{1}{n}\\cdot \\sum_{k=1}^{n_{samples}}\\ \\bigg[a[k]-y[k])\\bigg]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef528ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs :\n",
    "#   a : the activation values\n",
    "#   x : the features of each sample\n",
    "#   y : the targets\n",
    "\n",
    "# output :\n",
    "#   dw : the corrected weights\n",
    "#   db : the corrected bias\n",
    "\n",
    "def gradients(a, x, y):\n",
    "\n",
    "    db = 1 / n_samples * np.sum(a-y)\n",
    "    dw = 1 / n_samples * np.dot(x.T, a-y)\n",
    "        \n",
    "    return (db,dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa82da5",
   "metadata": {},
   "source": [
    "## 5/ Parameters update\n",
    "***\n",
    "the notations are :\n",
    "- $\\alpha$ : the learning rate\n",
    "\n",
    "\n",
    "the functions are :\n",
    "- *update* : to correct the bias and weights values at the learning_rate speed such as at the time $(t+1)$\n",
    "    - $w(t+1) = w(t) - \\alpha\\cdot \\delta w(t)$ <br>\n",
    "    - $b(t+1) = b(t) - \\alpha\\cdot \\delta b(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a7a34cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs :\n",
    "#   - b     : the bias at time (t)\n",
    "#   - w     : the weight at time (t)\n",
    "#   - db    : the bias correction at time (t)\n",
    "#   - dw    : the weights correction at time (t)\n",
    "#   - alpha : the learning rate\n",
    "\n",
    "# output :\n",
    "#   - b     : the new bias\n",
    "#   - w     : the new weights\n",
    "\n",
    "def update(b,w,db,dw, alpha):\n",
    "\n",
    "    b = b - alpha * db\n",
    "    w = w - alpha * dw\n",
    "        \n",
    "    return (b,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a310a71",
   "metadata": {},
   "source": [
    "## 6/ Perceptron\n",
    "***\n",
    "the notations are :\n",
    "- $n_{iterations}$ : the number of iterations to update the bias and weights\n",
    "\n",
    "the functions are :\n",
    "- *perceptron* : the whole algorithm simulating an artificial neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c8373c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs :\n",
    "#   x       : features\n",
    "#   y       : targets\n",
    "#   alpha   : learning rate\n",
    "#   n       : number of iterations\n",
    "\n",
    "# outputs :\n",
    "#   b       : the bias\n",
    "#   w       : the weights\n",
    "#   LL      : the log_loss vector\n",
    "#   plot    : LL = f(n)\n",
    "\n",
    "def perceptron(x, y, alpha, n,init_method):\n",
    "\n",
    "    b,w = initialisation(x,method=init_method)\n",
    "    LL  = np.zeros(n)\n",
    "\n",
    "    # for each iteration\n",
    "    for k in range(n):\n",
    "        # we calculate the neuron output\n",
    "        a       = model(x, b, w)\n",
    "\n",
    "        # we caculate the losses\n",
    "        LL[k]   = log_loss(a, y)\n",
    "\n",
    "        # we calculate the bias and weights corrections\n",
    "        db,dw   = gradients(a, x, y)\n",
    "\n",
    "        # we update the bias and weights\n",
    "        b,w     = update(b, w, db, dw, alpha)\n",
    "\n",
    "    '''\n",
    "    plt.plot(LL)\n",
    "    plt.xlabel(\"number of iterations\")\n",
    "    plt.ylabel(\"$\\mathcal{LL}$\")\n",
    "    plt.show()\n",
    "    '''\n",
    "    \n",
    "    return (b,w,LL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e50a9",
   "metadata": {},
   "source": [
    "## 7/ Convergence\n",
    "***\n",
    "the notations are :\n",
    "- $n_{cv}$ : number of iterations until convergence\n",
    "- $\\varepsilon_{\\delta}$ : the condition to define when the log-losses do not evolve significantly\n",
    "- $\\varepsilon_{\\mathcal{LL}}$ the condition to define when the log-losses is low enough\n",
    "- $\\delta \\mathcal{LL}$ : the $(\\mathcal{LL})$ slopes such as $\\delta \\mathcal{LL}= \\bigg[\\mathcal{LL}[1]-\\mathcal{LL}[0],\\cdots,\\mathcal{LL}[n_{iterations}]-\\mathcal{LL}[n_{iterations}-1]\\bigg]$\n",
    "\n",
    "the functions are :\n",
    "- *convergence* : to determine how many iterations are need to get the convergence which is defined as :<br>\n",
    "$convergence \\iff \\Bigg\\{\\delta\\mathcal{LL}[n_{cv}]\\leq \\varepsilon_\\delta\\cdot \\big(\\mathcal{LL}[n_{cv}]-\\mathcal{LL}[n_{cv}-1]\\big)\\ \\&\\ \\mathcal{LL}(n_{cv})\\leq \\varepsilon_\\mathcal{LL}\\bigg\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b01d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs :\n",
    "#   LL  : the log-loss values\n",
    "#   eps : the precision condition to state the convergence\n",
    "\n",
    "# outputs :\n",
    "#   dLL : the LL slopes\n",
    "#   n_cv: the number of iteration until needed to converge\n",
    "\n",
    "def convergence(LL,eps_d,eps_ll):\n",
    "\n",
    "    # computation of the slopes\n",
    "    dLL = np.abs([LL[k+1]-LL[k] for k in range(len(LL)-1)])\n",
    "\n",
    "    # we go through the losses elements\n",
    "    for k in range(len(LL)):\n",
    "\n",
    "        # once the convergence conditionS are verified\n",
    "        if ((dLL[k-1]<eps_d) and (LL[k]<eps_ll)):\n",
    "\n",
    "            # then we get our number of iterations needed to converge\n",
    "            n_cv = k\n",
    "            \n",
    "            # and we end the loop\n",
    "            return n_cv,dLL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ede73c",
   "metadata": {},
   "source": [
    "## 8/ Hyper-parameters $-$ learning rate\n",
    "***\n",
    "the notations are :\n",
    "- $n_values$ : the number of learning rate values to test\n",
    "- $\\alpha_{min,max}$ : respectively the boundaries of the learning rate\n",
    "- $\\Alpha$ : containing all the learning rates values\n",
    "- $Losses$ : contains the log-losses values for each learning rates values\n",
    "- $CVs$ : contains all the number of iterations needed for each learning rate values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e2a56",
   "metadata": {},
   "source": [
    "### 8.1/ init_method = NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31b7363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values    = 5\n",
    "alpha_min   = 1e-1\n",
    "alpha_max   = 1\n",
    "Alpha      = np.linspace(alpha_min,alpha_max,n_values)\n",
    "\n",
    "eps_d       = 1e-2\n",
    "eps_ll      = 0.3\n",
    "\n",
    "Losses      = []\n",
    "CVs         = []\n",
    "method      = \"null\"\n",
    "\n",
    "for k in range(n_values):\n",
    "    Losses.append(perceptron(x,y,Alpha[k],100,init_method=method)[2])\n",
    "    tmp = convergence(Losses[k],eps_d,eps_ll)[0]\n",
    "    CVs.append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f141d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if draw == 1:\n",
    "    plt.figure(dpi=resolution)\n",
    "\n",
    "    # 2-for loops in order to ease the differenciation of the curves on the plot\n",
    "\n",
    "    for k in range(0,int(n_values/2+0.5)):\n",
    "        plt.plot(Losses[k],linewidth = np.exp(-0.1*k),label=(r\"$\\alpha$ ={} $-$ n_cv={}\".format(np.round(Alpha[k],1),CVs[k])))\n",
    "        plt.scatter(CVs[k],Losses[k][int(CVs[k])])\n",
    "\n",
    "    for k in range(int(n_values/2+0.5),n_values):\n",
    "        plt.plot(Losses[k],'--',linewidth = np.exp(-0.1*k),label=(r\"$\\alpha$ ={} $-$ n_cv={}\".format(np.round(Alpha[k],1),CVs[k])))\n",
    "        plt.scatter(CVs[k],Losses[k][int(CVs[k])])\n",
    "\n",
    "    Title =r\"$\\mathcal{LL}=f(n_{iterations},\\alpha) -$ init method = \"+\"{}\".format(method)\n",
    "\n",
    "    plt.xlabel('number of iterations')\n",
    "    plt.ylabel(\"$\\mathcal{LL}$\")\n",
    "    plt.legend()\n",
    "    plt.title(Title)\n",
    "    plt.savefig(\"Loss_NULL.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6ea762",
   "metadata": {},
   "source": [
    "### 8.2/ init_method = RAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a6b6c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values    = 5\n",
    "alpha_min   = 1e-1\n",
    "alpha_max   = 1\n",
    "Alpha       = np.linspace(alpha_min,alpha_max,n_values)\n",
    "\n",
    "eps_d       = 1e-2\n",
    "eps_ll      = 0.3\n",
    "\n",
    "Losses      = []\n",
    "CVs         = []\n",
    "method      = 'rand'\n",
    "\n",
    "for k in range(n_values):\n",
    "    Losses.append(perceptron(x,y,Alpha[k],100,init_method=method)[2])\n",
    "    tmp = convergence(Losses[k],eps_d,eps_ll)[0]\n",
    "    CVs.append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "333e13ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if draw == 1:\n",
    "    plt.figure(dpi=resolution)\n",
    "\n",
    "    # 2-for loops in order to ease the differenciation of the curves on the plot\n",
    "\n",
    "    for k in range(0,int(n_values/2+0.5)):\n",
    "        plt.plot(Losses[k],linewidth = np.exp(-0.1*k),label=(r\"$\\alpha$ ={} $-$ n_cv={}\".format(np.round(Alpha[k],1),CVs[k])))\n",
    "        plt.scatter(CVs[k],Losses[k][int(CVs[k])])\n",
    "\n",
    "    for k in range(int(n_values/2+0.5),n_values):\n",
    "        plt.plot(Losses[k],'--',linewidth = np.exp(-0.1*k),label=(r\"$\\alpha$ ={} $-$ n_cv={}\".format(np.round(Alpha[k],1),CVs[k])))\n",
    "        plt.scatter(CVs[k],Losses[k][int(CVs[k])])\n",
    "\n",
    "    Title =r\"$\\mathcal{LL}=f(n_{iterations},\\alpha) -$ init method = \"+\"{}\".format(method)\n",
    "\n",
    "    plt.xlabel('number of iterations')\n",
    "    plt.ylabel(\"$\\mathcal{LL}$\")\n",
    "    plt.legend()\n",
    "    plt.title(Title)\n",
    "    plt.savefig(\"Loss_RAND.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d96747",
   "metadata": {},
   "source": [
    "## 9/ Validation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c051ebc6",
   "metadata": {},
   "source": [
    "### 9.1/ Spliting the dataset\n",
    "the notations are :\n",
    "- $n_{folds}$ : number of folds we want to generate\n",
    "- $x,y\\_ split$ : respectively, array containing all the subsets of the original $\\{x,y\\}$\n",
    "- $x,y\\_ copy$ : respectively, array containing a copy of the original $\\{x,y\\}$\n",
    "- $j$ : a counter to decrease the size of the $(x,y\\_ copy)$ each time we randomly take an element in it\n",
    "- $fold\\_ size$ : the number of samples in each folds\n",
    "- $fold\\_ x,y$ : temporary variables in which are saved the folds which will be concatenated into the $(x,y\\_ split)$\n",
    "- $index$ : randomly chosen index of the element to add to the $(fold\\_ x,y)$ array\n",
    "- $X,Y\\_s$ : final training and validation sets\n",
    "\n",
    "the functions are:\n",
    "- *cross_validation_split* : to split the original dataset into 2 sets :\n",
    "    - 1st set : training set\n",
    "    - 2nd set : validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e4969d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs :\n",
    "#\tx,y\t\t: the original dataset\n",
    "# \tn_folds\t: the number of folds desired\n",
    "\n",
    "# outputs :\n",
    "#\tx,y_split\t: the splitted datasets\n",
    "\n",
    "def cross_validation_split(x,y,n_folds):\n",
    "\n",
    "\t# variables declarations\n",
    "\tx_split \t\t= []\n",
    "\ty_split \t\t= []\n",
    "\tx_copy \t\t\t= list(x)\n",
    "\ty_copy \t\t\t= list(y)\n",
    "\tj\t\t\t\t= -1\n",
    "\n",
    "\t# calculation of size of each folds\n",
    "\tfold_size \t\t= int(n_samples/n_folds)\n",
    "\n",
    "\t# for each folds\n",
    "\tfor k in range(n_folds):\n",
    "\n",
    "\t\t# temporary folds\n",
    "\t\tfold_x\t\t= []\n",
    "\t\tfold_y\t\t= []\n",
    "\n",
    "\t\t# while the fold is not full\n",
    "\t\twhile len(fold_x)<fold_size:\n",
    "\t\t\tj\t\t= j+1\n",
    "\n",
    "\t\t\t# we randomly chose an index of the element to withdraw from the copies of the original sets\n",
    "\t\t\tindex \t= randrange(n_samples-j)\n",
    "\n",
    "\t\t\t# then we add the corresponding element into the temporary folds\n",
    "\t\t\tfold_x.append(x_copy.pop(index))\n",
    "\t\t\tfold_y.append(y_copy.pop(index))\n",
    "\n",
    "\t\t# we concatenate the subsets\n",
    "\t\tx_split.append(fold_x)\n",
    "\t\ty_split.append(fold_y)\n",
    "\t\n",
    "\t# sizes manipulations to enable data processing by the perceptron algorithm\n",
    "\t# Intial splitted datasets\n",
    "\tX_si, Y_si = np.array(x_split),np.array(y_split)\n",
    "\n",
    "\t# splitted dataset for Training\n",
    "\tX_s0    = np.array([X_si[k][:,0] for k in range(n_folds-1)]).flatten()[np.newaxis,:]\n",
    "\tX_s1    = np.array([X_si[k][:,1] for k in range(n_folds-1)]).flatten()[np.newaxis,:]\n",
    "\tX_st    = np.concatenate((X_s0,X_s1),axis = 0)[np.newaxis,:][0]\n",
    "\tY_st    = np.array([Y_si[k][:,0] for k in range(n_folds-1)]).flatten()\n",
    "\n",
    "\t# splitted dataset for Validation\n",
    "\tX_sv\t= X_si[-1]\n",
    "\tY_sv\t= Y_si[-1]\n",
    "\n",
    "\treturn X_st.T,Y_st[np.newaxis,:].T,X_sv,Y_sv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69cecce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "method              = \"rand\"\n",
    "n_folds             = 3\n",
    "learning_rate       = 1\n",
    "n_iterations        = 100\n",
    "\n",
    "X_st,Y_st,X_sv,Y_sv = cross_validation_split(x,y,n_folds)\n",
    "fig_length = 5\n",
    "fig_height = fig_length/1.618\n",
    "\n",
    "if draw == 1:\n",
    "    plt.figure(dpi=resolution)\n",
    "    plt.scatter(X_st[0],X_st[1],s=fig_length*10,c=Y_st,label=\"training\")\n",
    "    plt.scatter(X_sv.T[0],X_sv.T[1],c=Y_sv.flatten(),s=fig_length/2,alpha = 0.75,label=\"validation\")\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.title(\"Visualisation of the training & validation datasets\\n\",fontsize=15)\n",
    "    plt.suptitle(\"the colors show in what class the data should belong\",fontsize=8,y=0.92)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"training_and_validation_datasets.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8380c11f",
   "metadata": {},
   "source": [
    "### 9.2/ Training on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb74b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "B,W,LL = perceptron(X_st,Y_st,alpha=0.4,n=100,init_method='null')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd5fe68",
   "metadata": {},
   "source": [
    "### 9.3/ Prediction on validation dataset\n",
    "the notations are :\n",
    "- *model_output* : ...\n",
    "- $threshold$ : if the model_output > threshold, then decide 1. Else, decide 0\n",
    "- *predictions* : ...\n",
    "\n",
    "the functions are :\n",
    "- *decision* : decides if the input data belongs to one or another class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1397240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs :\n",
    "#\n",
    "def decision(model_output,threshold):\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    for k in range(len(model_output)):\n",
    "        if model_output[k]>threshold:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce82ff7",
   "metadata": {},
   "source": [
    "## 10/ Performance evaluation\n",
    "***\n",
    "the notations are :\n",
    "- *a*\n",
    "\n",
    "the functions are :\n",
    "- *confusion_matrix* : draws the confusion matrix\n",
    "- *performance_metrics* : calculates the basic metrics :\n",
    "    - $accuracy$\n",
    "    - $precision$\n",
    "    - $F_1-score$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
